{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5be8f26-fa62-4646-9b90-2d56ce0cd143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "from sort import Sort\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a85a0-2f53-4c59-9b53-703f50041937",
   "metadata": {},
   "source": [
    "## 10.1.1 모션벡터와 광류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ada2577-6425-445e-be6c-073222c55d77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82104\\AppData\\Local\\Temp\\ipykernel_7740\\1357648497.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dx,dy=flow[y,x].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# Farneback 알고리즘으로 광류 추정\n",
    "def draw_OpticalFlow(img,flow,step=16):\n",
    "    for y in range(step//2,frame.shape[0],step):\n",
    "        for x in range(step//2,frame.shape[1],step):\n",
    "            dx,dy=flow[y,x].astype(np.int)\n",
    "            if(dx*dx+dy*dy)>1:\n",
    "                cv2.line(img,(x,y),(x+dx,y+dy),(0,0,255),2) \n",
    "            else:\n",
    "                cv2.line(img,(x,y),(x+dx,y+dy),(0,255,0),2)            \n",
    "    \n",
    "cap=cv2.VideoCapture(\"./video/boy.mov\")\t\n",
    "if not cap.isOpened(): sys.exit('카메라 연결 실패')\n",
    "    \n",
    "prev=None\n",
    "\n",
    "while(1):\n",
    "    ret,frame=cap.read()\t\n",
    "    if not ret: sys('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "    \n",
    "    if prev is None:\t\n",
    "        prev=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        continue\n",
    "    \n",
    "    curr=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    flow=cv2.calcOpticalFlowFarneback(prev,curr,None,0.5,3,15,3,5,1.2,0)\n",
    "    \n",
    "    draw_OpticalFlow(frame,flow)\n",
    "    cv2.imshow('Optical flow',frame)\n",
    "\n",
    "    prev=curr\n",
    "\n",
    "    key=cv2.waitKey(1)\t\n",
    "    if key==ord('q'):\t\n",
    "        break \n",
    "    \n",
    "cap.release()\t\t\t\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e5901c-d35b-4274-9755-47449e23722d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 희소 광류 추정을 이용한 KLT 추적\n",
    "cap=cv2.VideoCapture('./video/slow_traffic_small.mp4')\n",
    "\n",
    "feature_params=dict(maxCorners=100,qualityLevel=0.3,minDistance=7,blockSize=7)\n",
    "lk_params=dict(winSize=(15,15),maxLevel=2,criteria=(cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_COUNT,10,0.03))\n",
    "\n",
    "color=np.random.randint(0,255,(100,3))\n",
    "\n",
    "ret,old_frame=cap.read()\t\t\n",
    "old_gray=cv2.cvtColor(old_frame,cv2.COLOR_BGR2GRAY)\n",
    "p0=cv2.goodFeaturesToTrack(old_gray,mask=None,**feature_params)\n",
    "\n",
    "mask=np.zeros_like(old_frame)\t \n",
    "\n",
    "while(1):\n",
    "    ret,frame=cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    new_gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    p1,match,err=cv2.calcOpticalFlowPyrLK(old_gray,new_gray,p0,None,**lk_params)\t\n",
    "    \n",
    "    if p1 is not None:\t\t\n",
    "        good_new=p1[match==1]\n",
    "        good_old=p0[match==1]\n",
    "        \n",
    "    for i in range(len(good_new)): \n",
    "        a,b=int(good_new[i][0]),int(good_new[i][1])\n",
    "        c,d=int(good_old[i][0]),int(good_old[i][1])\n",
    "        mask=cv2.line(mask,(a,b),(c,d),color[i].tolist(),2)\n",
    "        frame=cv2.circle(frame,(a,b),5,color[i].tolist(),-1)\n",
    "        \n",
    "    img=cv2.add(frame,mask)\n",
    "    cv2.imshow('LTK tracker',img)\n",
    "    cv2.waitKey(30)\n",
    "\n",
    "    old_gray=new_gray.copy()\t\n",
    "    p0=good_new.reshape(-1,1,2)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c415ed-f67d-4f2d-8ef3-8777bda92fc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 10.2.4 프로그래밍 실습: SORT로 사람 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90ae84a-007f-4856-bea9-df7da0d7bb62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_yolo_v3():\n",
    "    f=open('./saved_model/coco_names.txt', 'r')\n",
    "    class_names=[line.strip() for line in f.readlines()]\n",
    "\n",
    "    model=cv2.dnn.readNet('./saved_model/yolov3.weights','./saved_model/yolov3.cfg')\n",
    "    layer_names=model.getLayerNames()\n",
    "    out_layers=[layer_names[i-1] for i in model.getUnconnectedOutLayers()]\n",
    "    \n",
    "    return model,out_layers,class_names\n",
    "\n",
    "def yolo_detect(img,yolo_model,out_layers):\n",
    "    height,width=img.shape[0],img.shape[1]\n",
    "    test_img=cv2.dnn.blobFromImage(img,1.0/256,(448,448),(0,0,0),swapRB=True)\n",
    "    \n",
    "    yolo_model.setInput(test_img)\n",
    "    output3=yolo_model.forward(out_layers)\n",
    "    \n",
    "    box,conf,id=[],[],[]\t\t\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores=vec85[5:]\n",
    "            class_id=np.argmax(scores)\n",
    "            confidence=scores[class_id]\n",
    "            if confidence>0.5:\t\n",
    "                centerx,centery=int(vec85[0]*width),int(vec85[1]*height)\n",
    "                w,h=int(vec85[2]*width),int(vec85[3]*height)\n",
    "                x,y=int(centerx-w/2),int(centery-h/2)\n",
    "                box.append([x,y,x+w,y+h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "            \n",
    "    ind=cv2.dnn.NMSBoxes(box,conf,0.5,0.4)\n",
    "    objects=[box[i]+[conf[i]]+[id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "model,out_layers,class_names=construct_yolo_v3()\t\n",
    "colors=np.random.uniform(0,255,size=(100,3))\t\t\n",
    "\n",
    "sort=Sort()\n",
    "\n",
    "cap=cv2.VideoCapture(\"./video/slow_traffic_small.mp4\")\n",
    "if not cap.isOpened(): sys.exit('카메라 연결 실패')\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret: sys.exit('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        \n",
    "    res=yolo_detect(frame,model,out_layers)   \n",
    "    persons=[res[i] for i in range(len(res)) if res[i][5]==0] \n",
    "\n",
    "    if len(persons)==0: \n",
    "        tracks=sort.update()\n",
    "    else:\n",
    "        tracks=sort.update(np.array(persons))\n",
    "    \n",
    "    for i in range(len(tracks)):\n",
    "        x1,y1,x2,y2,track_id=tracks[i].astype(int)\n",
    "        cv2.rectangle(frame,(x1,y1),(x2,y2),colors[track_id],2)\n",
    "        cv2.putText(frame,str(track_id),(x1+10,y1+40),cv2.FONT_HERSHEY_PLAIN,3,colors[track_id],2)            \n",
    "    \n",
    "    cv2.imshow('Person tracking by SORT',frame)\n",
    "    \n",
    "    key=cv2.waitKey(1) \n",
    "    if key==ord('q'): break \n",
    "    \n",
    "cap.release()\t\t\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a8e61-bd15-4834-a9df-2409003e9cc0",
   "metadata": {},
   "source": [
    "### 10.3.1 얼굴 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "252a3e7d-5d53-4238-bd48-ff071bfebe06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 정지 영상\n",
    "img=cv2.imread('./imgs/BSDS_376001.jpg')\n",
    "\n",
    "mp_face_detection=mp.solutions.face_detection\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "\n",
    "face_detection=mp_face_detection.FaceDetection(model_selection=1,min_detection_confidence=0.5)\n",
    "res=face_detection.process(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "if not res.detections:\n",
    "    print('얼굴 검출에 실패했습니다. 다시 시도하세요.')\n",
    "else:\n",
    "    for detection in res.detections:\n",
    "        mp_drawing.draw_detection(img,detection)\n",
    "    cv2.imshow('Face detection by MediaPipe',img)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49adc36b-a366-4824-a396-bca57265b064",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프레임 획득에 실패하여 루프를 나갑니다.\n"
     ]
    }
   ],
   "source": [
    "# 비디오\n",
    "mp_face_detection=mp.solutions.face_detection\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "\n",
    "face_detection=mp_face_detection.FaceDetection(model_selection=1,min_detection_confidence=0.5)\n",
    "\n",
    "cap=cv2.VideoCapture(\"./video/face.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        break\n",
    "    \n",
    "    res=face_detection.process(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    if res.detections:\n",
    "        for detection in res.detections:\n",
    "            mp_drawing.draw_detection(frame,detection)\n",
    "            \n",
    "    cv2.imshow('MediaPipe Face Detection from video',cv2.flip(frame,1))\n",
    "    if cv2.waitKey(5)==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69848c91-a981-4448-a148-643ef02a8cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 증강 현실\n",
    "dice=cv2.imread('./imgs/dice.png',cv2.IMREAD_UNCHANGED)\n",
    "dice=cv2.resize(dice,dsize=(0,0),fx=0.1,fy=0.1)\n",
    "w,h=dice.shape[1],dice.shape[0]\n",
    "\n",
    "mp_face_detection=mp.solutions.face_detection\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "\n",
    "face_detection=mp_face_detection.FaceDetection(model_selection=1,min_detection_confidence=0.5)\n",
    "\n",
    "cap=cv2.VideoCapture(\"./video/face.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        break\n",
    "    \n",
    "    res=face_detection.process(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    if res.detections:\n",
    "        for det in res.detections:\n",
    "            p=mp_face_detection.get_key_point(det,mp_face_detection.FaceKeyPoint.RIGHT_EYE)\n",
    "            x1,x2=int(p.x*frame.shape[1]-w//2),int(p.x*frame.shape[1]+w//2)\n",
    "            y1,y2=int(p.y*frame.shape[0]-h//2),int(p.y*frame.shape[0]+h//2)\n",
    "            if x1>0 and y1>0 and x2<frame.shape[1] and y2<frame.shape[0]:\n",
    "                alpha=dice[:,:,3:]/255\n",
    "                frame[y1:y2,x1:x2]=frame[y1:y2,x1:x2]*(1-alpha)+dice[:,:,:3]*alpha\n",
    "            \n",
    "    cv2.imshow('MediaPipe Face AR',cv2.flip(frame,1))\n",
    "    if cv2.waitKey(5)==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64161757-12c0-421a-93dc-6db09a7b770e",
   "metadata": {},
   "source": [
    "### 10.3.2 얼굴 그물망 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f712ecba-1d38-43f3-b3eb-95ce23eff5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp_mesh=mp.solutions.face_mesh\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_styles=mp.solutions.drawing_styles\n",
    "\n",
    "mesh=mp_mesh.FaceMesh(max_num_faces=2,refine_landmarks=True,min_detection_confidence=0.1,min_tracking_confidence=0.1)\n",
    "\n",
    "cap=cv2.VideoCapture(\"./video/face2.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        break\n",
    "    \n",
    "    res=mesh.process(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    if res.multi_face_landmarks:\n",
    "        for landmarks in res.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(image=frame,landmark_list=landmarks,connections=mp_mesh.FACEMESH_TESSELATION,landmark_drawing_spec=None,connection_drawing_spec=mp_styles.get_default_face_mesh_tesselation_style())\n",
    "            mp_drawing.draw_landmarks(image=frame,landmark_list=landmarks,connections=mp_mesh.FACEMESH_CONTOURS,landmark_drawing_spec=None,connection_drawing_spec=mp_styles.get_default_face_mesh_contours_style())\n",
    "            mp_drawing.draw_landmarks(image=frame,landmark_list=landmarks,connections=mp_mesh.FACEMESH_IRISES,landmark_drawing_spec=None,connection_drawing_spec=mp_styles.get_default_face_mesh_iris_connections_style())\n",
    "        \n",
    "    cv2.imshow('MediaPipe Face Mesh',cv2.flip(frame,1))\n",
    "    if cv2.waitKey(5)==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6fcd90-ba45-4658-85a9-168412df39ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 10.3.3 손 랜드마드 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ef182c0-c6ea-4317-8f4e-43c73949d321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp_hand=mp.solutions.hands\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_styles=mp.solutions.drawing_styles\n",
    "\n",
    "hand=mp_hand.Hands(max_num_hands=2,static_image_mode=False,min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "\n",
    "cap=cv2.VideoCapture(\"./video/hand.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        break\n",
    "    \n",
    "    res=hand.process(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    if res.multi_hand_landmarks:\n",
    "        for landmarks in res.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame,landmarks,mp_hand.HAND_CONNECTIONS,mp_styles.get_default_hand_landmarks_style(),mp_styles.get_default_hand_connections_style())\n",
    "\n",
    "    cv2.imshow('MediaPipe Hands',cv2.flip(frame,1))\n",
    "    if cv2.waitKey(5)==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0877a8ec-b4ad-4e12-8f5e-64ffe16b076f",
   "metadata": {},
   "source": [
    "### 10.4.2 BlazePose를 이용한 자세 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7354c7b7-2ad5-4f46-af0c-14557262db5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp_pose=mp.solutions.pose\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_styles=mp.solutions.drawing_styles\n",
    "\n",
    "pose=mp_pose.Pose(static_image_mode=False,enable_segmentation=True,min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "\n",
    "cap=cv2.VideoCapture(\"./video/face2.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        break\n",
    "    \n",
    "    res=pose.process(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    mp_drawing.draw_landmarks(frame,res.pose_landmarks,mp_pose.POSE_CONNECTIONS,landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style())\n",
    "    \n",
    "    cv2.imshow('MediaPipe pose',cv2.flip(frame,1))\n",
    "    if cv2.waitKey(5)==ord('q'):\n",
    "        mp_drawing.plot_landmarks(res.pose_world_landmarks,mp_pose.POSE_CONNECTIONS)\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
